{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ComplaintsDataset is our initial full dataset. Problem_Data is the ComplaintsDataset but after preprocessing and contains all the complaints classified in 5 categories. Problem_Data_512 contains the same minus the complaints with length greater than 512. Load dataset drop empty rows and duplicates. We don't need the labels in this notebook, but watch out when we group them together, the result of grouped categories will be different depending on the preprocessing"
      ],
      "metadata": {
        "id": "Kn4LT5zr8pY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB2e4YmCcnui",
        "outputId": "95b8b8a2-672f-4917-f206-d7408393d3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # display the whole dataframe rows\n",
        "# pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# # display all dataframe rows\n",
        "# pd.set_option('display.max_rows', None)\n",
        "\n",
        "datapath = 'ComplaintsDataset.csv'\n",
        "\n",
        "data = pd.read_csv(datapath)\n",
        "\n",
        "data = data[['Λεπτομέρειες','Υποενότητα']].rename(columns={'Λεπτομέρειες':'text','Υποενότητα':'label'})\n",
        "\n",
        "# drop all rows with NaN values\n",
        "data.dropna(subset=['text'], inplace=True)\n",
        "print('NaN values in text column: ',data['text'].isna().sum())\n",
        "\n",
        "data = data.drop(['label'], axis=1)\n",
        "\n",
        "# # Filter and print duplicate rows\n",
        "# print(data['text'][data['text'].duplicated(keep=False)])\n",
        "data = data.drop_duplicates(subset=['text'])\n",
        "print(\"Number of duplicates:\", data['text'].duplicated().sum())\n",
        "\n",
        "# make column with original complaints\n",
        "data = data.assign(init_text=data['text'])\n",
        "\n",
        "# data=data.dropna()\n",
        "data = data.reset_index(drop = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO-2ZDSZiIqn",
        "outputId": "4de786ac-52c5-4d07-d851-bf85cde92abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in text column:  0\n",
            "Number of duplicates: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# #frequency of each label\n",
        "# label_counts = data['label'].value_counts()\n",
        "# # Merge all low-frequency classes into the class \"Άλλο\"\n",
        "# low_freq_labels = label_counts[label_counts < 380].index.tolist()\n",
        "# data.loc[data['label'].isin(low_freq_labels), 'label'] = 'Άλλο'\n",
        "# # Save labels in a list\n",
        "# label_list = data[\"label\"].unique()\n",
        "# # Map the labels to integers\n",
        "# label_ids = {label: i for i, label in enumerate(label_list)}\n",
        "# #frequency of each label\n",
        "# label_counts = data['label'].value_counts()\n",
        "# print(label_counts)\n",
        "# # # Replace label names with their corresponding values\n",
        "# # data['label'] = [label_ids[label] for label in data['label']]\n",
        "\n",
        "# strip accents and lowercase function\n",
        "def strip_accents_and_lowercase(s):\n",
        "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                  if unicodedata.category(c) != 'Mn').lower()\n",
        "\n",
        "data['text'] = data['text'].apply(strip_accents_and_lowercase)\n",
        "\n",
        "# remove stop words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('greek'))\n",
        "\n",
        "# Define a function to remove stopwords, special symbols, and tokenize the text\n",
        "additional_stopwords = [\"απο\",\"από\", \"μου\", \"στις\", \"ειναι\",\"είναι\", \"καθώς\",\"μας\",\"ότι\",\"ήταν\",\"αλλά\",\"ενώ\",\"τους\",\"αυτό\",\"εχει\",\"πρέπει\",\"ή\",\"γιατί\", \"όπως\",\"ούτε\", \"όταν\", \"χωρίς\", \"ένα\", \"αφού\", \"έχει\", \"είχε\"]\n",
        "def remove_stopwords(text):\n",
        "    # Remove special symbols using regex\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove links and URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Tokenize the text\n",
        "    # words = word_tokenize(text)\n",
        "    words = text.split()\n",
        "    # words = [word for word in words if word.lower() not in stop_words]\n",
        "    words = [word for word in words if word.lower() not in stop_words and word.lower() not in additional_stopwords]\n",
        "    return \" \".join(words)\n",
        "\n",
        "data['text'] = data['text'].apply(remove_stopwords)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "QKUpZR5dk22t",
        "outputId": "f1d4aad4-acd0-4902-903a-2e62e79743be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0     καλημερα σας κατοχος της υπαριθμον ανωνυμη καρ...   \n",
              "1     αγαπητοι κυριοι καλησπερα σας σημερα ωρα απογε...   \n",
              "2     στης μπηκα λεωφορειο παω τερμα δυο στασεις προ...   \n",
              "3     σταθμος τραμ γηπεδο καραισκακη τηλεφωνο εκτακτ...   \n",
              "4     σταθμος κεραμεικος γραμμη επιχειρησα ανανεωσω ...   \n",
              "...                                                 ...   \n",
              "6637  καλημερα σας παρακαλω ηθελα ενημερωθω ακυρωση ...   \n",
              "6638  καλημερα σας καλο μηνα λεωφορειο τερμα της ζωο...   \n",
              "6639  καλημερα σας απαραδεκτος οδηγος καθως κοπελα η...   \n",
              "6640                 δρομολογιο ενημερωστε γιατι περασε   \n",
              "6641  αξιοτιμη υπηρεσια ηθελα καταγγειλω δυστυχως εν...   \n",
              "\n",
              "                                              init_text  \n",
              "0     Καλημερα σας  ειμαι κατοχος της υπαριθμον  300...  \n",
              "1     Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...  \n",
              "2     στης 19.01 μπήκα στο λεωφορείο 022 για να παω ...  \n",
              "3     Σταθμός Τραμ Γήπεδο Καραϊσκάκη. Το τηλέφωνο έκ...  \n",
              "4     Σταθμός Κεραμεικός, γραμμή 3 . Επιχείρησα να α...  \n",
              "...                                                 ...  \n",
              "6637  Καλημέρα σας!\\n\\nΠαρακαλώ θα ήθελα να ενημερωθ...  \n",
              "6638  Καλημέρα σας και καλό μηνα. Το λεωφορείο 730 α...  \n",
              "6639  Καλημέρα σας, \\nΑπαράδεκτος οδηγός, καθώς η κο...  \n",
              "6640  Το δρομολόγιο των 7 και 20 ενημερώστε με γιατί...  \n",
              "6641  Αξιότιμη Υπηρεσία,\\nΘα ήθελα να καταγγείλω, δυ...  \n",
              "\n",
              "[6642 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2401e17-449a-46ce-abc0-4fc3abbdf4ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>init_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>καλημερα σας κατοχος της υπαριθμον ανωνυμη καρ...</td>\n",
              "      <td>Καλημερα σας  ειμαι κατοχος της υπαριθμον  300...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>αγαπητοι κυριοι καλησπερα σας σημερα ωρα απογε...</td>\n",
              "      <td>Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>στης μπηκα λεωφορειο παω τερμα δυο στασεις προ...</td>\n",
              "      <td>στης 19.01 μπήκα στο λεωφορείο 022 για να παω ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>σταθμος τραμ γηπεδο καραισκακη τηλεφωνο εκτακτ...</td>\n",
              "      <td>Σταθμός Τραμ Γήπεδο Καραϊσκάκη. Το τηλέφωνο έκ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>σταθμος κεραμεικος γραμμη επιχειρησα ανανεωσω ...</td>\n",
              "      <td>Σταθμός Κεραμεικός, γραμμή 3 . Επιχείρησα να α...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6637</th>\n",
              "      <td>καλημερα σας παρακαλω ηθελα ενημερωθω ακυρωση ...</td>\n",
              "      <td>Καλημέρα σας!\\n\\nΠαρακαλώ θα ήθελα να ενημερωθ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6638</th>\n",
              "      <td>καλημερα σας καλο μηνα λεωφορειο τερμα της ζωο...</td>\n",
              "      <td>Καλημέρα σας και καλό μηνα. Το λεωφορείο 730 α...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6639</th>\n",
              "      <td>καλημερα σας απαραδεκτος οδηγος καθως κοπελα η...</td>\n",
              "      <td>Καλημέρα σας, \\nΑπαράδεκτος οδηγός, καθώς η κο...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6640</th>\n",
              "      <td>δρομολογιο ενημερωστε γιατι περασε</td>\n",
              "      <td>Το δρομολόγιο των 7 και 20 ενημερώστε με γιατί...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6641</th>\n",
              "      <td>αξιοτιμη υπηρεσια ηθελα καταγγειλω δυστυχως εν...</td>\n",
              "      <td>Αξιότιμη Υπηρεσία,\\nΘα ήθελα να καταγγείλω, δυ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6642 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2401e17-449a-46ce-abc0-4fc3abbdf4ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2401e17-449a-46ce-abc0-4fc3abbdf4ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2401e17-449a-46ce-abc0-4fc3abbdf4ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counts texts with more than 512 tokens and then discards them"
      ],
      "metadata": {
        "id": "4ZOBcw3v7-2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import AutoTokenizer\n",
        "# # Load greek BERT tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained('nlpaueb/bert-base-greek-uncased-v1')\n",
        "# #tokenize texts and find number of texts with more than 512 tokens\n",
        "# LongTextsCount = 0\n",
        "# for text in data['text']:\n",
        "#     tokenized = tokenizer.encode(text, truncation=False,return_tensors='pt', add_special_tokens=True)\n",
        "#     if tokenized.size()[1] > 512:\n",
        "#         LongTextsCount += 1\n",
        "# print(\"\\n\\nNumber of texts with more than 512 tokens: \", LongTextsCount)\n",
        "# # Drop long texts since there's too few of them\n",
        "# LongTextIdx = []\n",
        "# for i, text in enumerate(data['text']):\n",
        "#     tokenized = tokenizer.encode(text, truncation=False, return_tensors='pt', add_special_tokens=True)\n",
        "#     if tokenized.size()[1] > 512:\n",
        "#         LongTextIdx.append(i)\n",
        "# data_rem = data.drop(index=LongTextIdx).reset_index(drop=True)\n",
        "# data_rem"
      ],
      "metadata": {
        "id": "b57RhdYMtMCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make a list of words and search for them, or for words that partially match with them, inside the texts - keep only the sentences that contain such words"
      ],
      "metadata": {
        "id": "yWctbXder1kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# words that we are looking for - 2 categories\n",
        "strings_law = ['δικαστ', 'νομικ', 'αγωγ', 'αποζημι', 'αποζημί', 'δικαιοσ', 'αδικημ', 'αδίκημ', 'δίκη', 'μηνυσ', 'μήνυσ']\n",
        "strings_hurt = ['τραυμ', 'διαστρεμ', 'διάστρεμ', 'κακωση', 'κάκωση', 'κάταγμα', 'βάρεσ', 'βαρά', 'βαραει', 'βαραω', 'βαρεσ',\n",
        "                'καταγμα', 'χτυπ', 'χτύπ', 'νοσοκομ', 'επίθεσ', 'επιθεσ', 'επιτεθ', 'επιθετ', 'επιτέθ']\n",
        "\n",
        "# keep only texts that contain keywords but beware that words\n",
        "# can only start with our strings to keep and not end with them\n",
        "# for example \"ξαναχτυπάω\" won't match\n",
        "pattern = r'(?<=[\\s\\W])(?:' + '|'.join(map(re.escape, strings_law + strings_hurt)) + r')'\n",
        "filtered_df = data[data['text'].str.contains(pattern, regex=True)]\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# our texts might have dots or other symbols that are immediately followed by string with\n",
        "# no space. the below goes through the text and adds a space where such a thing occurs\n",
        "def add_space_after_symbol(df, column_name):\n",
        "    new_df = df.copy()\n",
        "    pattern = r'[-.,()…](?=\\S)'\n",
        "    # Function to add a space after the symbol\n",
        "    def add_space(match):\n",
        "        return match.group(0) + ' '\n",
        "    new_df[column_name] = new_df[column_name].apply(lambda x: re.sub(pattern, add_space, x))\n",
        "    return new_df\n",
        "\n",
        "filtered_df = add_space_after_symbol(filtered_df, \"text\")\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# # def find_occurrence(text, search_string, k):\n",
        "# #     pattern = r\"(?<!\\w){}(?!\\w)\".format(re.escape(search_string))\n",
        "# #     matches = re.findall(pattern, text)\n",
        "# #     print(\"matches \", matches)\n",
        "# #     if len(matches) < k:\n",
        "# #         return None\n",
        "# #     start_index = text.find(matches[k - 1])\n",
        "# #     end_index = start_index + len(matches[k - 1]) - 1\n",
        "# #     return (start_index, end_index)\n",
        "# # find the start and end position of a string in a particular\n",
        "# # text take into account that some times the same word\n",
        "# # can be counted in the matches more than one time\n",
        "# def find_occurrence(search_string, text, k):\n",
        "#     start_index = text.find(search_string)\n",
        "#     while start_index >= 0 and k > 1:\n",
        "#         start_index = text.find(search_string, start_index + 1)\n",
        "#         k -= 1\n",
        "#     if start_index < 0:\n",
        "#         return None\n",
        "#     end_index = start_index + len(search_string) - 1\n",
        "#     return (start_index, end_index)\n",
        "# def print_substring(text, start, end):\n",
        "#     substring = text[start:end+1]\n",
        "#     print(substring)\n",
        "# # might not be used. replaces specific characters with space\n",
        "# def replace_characters_with_space(text):\n",
        "#     characters = [\"»\", \"…\"]\n",
        "#     for char in characters:\n",
        "#         text = text.replace(char, ' ')\n",
        "#     return text\n",
        "# # check if a token is space or Punctuation\n",
        "# def is_space_or_punctuation(char):\n",
        "#     return char.isspace() or char in string.punctuation or char==\"»\" or char==\"…\"\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# return the position number of the beginning of the immediate identical match of string in a text\n",
        "def find_next_identical_match(text, my_string):\n",
        "    pattern = r'\\b{}\\b'.format(re.escape(my_string))\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return match.start()\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# returns the beginning and end position numbers of the k-th ocurrence of a string in a text\n",
        "def ret_beginning_and_end_of_kth_occurence(my_txt, search_string, k):\n",
        "    # my_txt = replace_characters_with_space(my_txt)\n",
        "    start = find_next_identical_match(my_txt, search_string)\n",
        "    while start >= 0 and k > 1:\n",
        "        start = start + len(search_string) + find_next_identical_match(my_txt[start + len(search_string):], search_string)\n",
        "        k -= 1\n",
        "    return start, start + len(search_string)-1\n",
        "\n",
        "# takes two strings and returns True if they are same. beware that if str1 is\n",
        "# same with str2 but also contains punctuation, it will also be considered same\n",
        "def compare_strings_forget_punct(str1, str2):\n",
        "    # Concatenate the standard punctuation and additional punctuations\n",
        "    additional_punctuations = \"[]{}()!@#$%^&*+=_-|\\/?.,;:…\"\n",
        "    all_punctuations = string.punctuation + additional_punctuations\n",
        "    translator = str.maketrans(\"\", \"\", all_punctuations)\n",
        "    str1 = str1.translate(translator)\n",
        "    str2 = str2.translate(translator)\n",
        "    return str1 == str2\n",
        "\n",
        "# takes a list of words and a word and returns the index\n",
        "# of the word in the list using the compare_strings_forget_punct\n",
        "def get_word_index_from_list(list_words, word):\n",
        "  counter = 0\n",
        "  while counter <= len(list_words):\n",
        "      if compare_strings_forget_punct(list_words[counter], word):\n",
        "          return counter\n",
        "      else:\n",
        "          counter += 1\n",
        "\n",
        "# given a text, a word and a number k, return the word index of the k-th word occurence in the text\n",
        "def find_word_serial_number(text, word, k):\n",
        "    # the number of previous same words\n",
        "    offset_words = 0\n",
        "    words = text.split()\n",
        "    while k>0:\n",
        "      try:\n",
        "          # serial_number = words.index(word) + 1\n",
        "          serial_number = get_word_index_from_list(words, word)\n",
        "          k -= 1\n",
        "          if k == 0:\n",
        "            return serial_number + offset_words\n",
        "          else:\n",
        "            offset_words += serial_number + 1\n",
        "            # keep only the remaining words in the list discard the previous\n",
        "            # since we have already counted them\n",
        "            words = words[serial_number+1:]\n",
        "      except ValueError:\n",
        "          return -1\n",
        "\n",
        "# starting from the dataframe texts we keep, go further and only keep sentences\n",
        "# which contain strings to keep beware though that we will discard sentences in\n",
        "# which we find words like 'χτυπάω', 'χτύπησα' e.g. and they refer to tickets\n",
        "def filter_sentences_discard_tickets(text, keywords):\n",
        "    related_to_tickets = ['χτυπ', 'χτύπ']\n",
        "    ticket_strings = ['εισητ', 'εισιτ', 'εισειτ', 'καρτ', 'κάρτ']\n",
        "    filtered_sentences = []\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for keyword in keywords:\n",
        "            # now check if those sentences contain texts like 'χτυπαω εισητηριο'\n",
        "            if keyword in related_to_tickets:\n",
        "                pattern = r\"\\b{}\\w*\\b\".format(keyword)\n",
        "                cond = re.search(pattern, sentence, flags=re.IGNORECASE)\n",
        "                keep_sen = True\n",
        "                if cond:\n",
        "                  matches = re.findall(pattern, sentence)\n",
        "                  for word_ind in range(len(matches)):\n",
        "\n",
        "                      # # find position of first and last tokens of word in sentence\n",
        "                      # word_start, word_end = ret_beginning_and_end_of_kth_occurence(sentence, matches[word_ind], word_ind+1)\n",
        "\n",
        "                      # number of same words in the matches list before the current one\n",
        "                      count_found = 1\n",
        "                      previous_words = matches[:word_ind]\n",
        "                      for i in previous_words:\n",
        "                          if compare_strings_forget_punct(i, matches[word_ind]):\n",
        "                              count_found += 1\n",
        "\n",
        "                      # get word index in text\n",
        "                      word_in_text = find_word_serial_number(sentence, matches[word_ind], count_found)\n",
        "\n",
        "                      # remove punctuation\n",
        "                      sentence_no_pun = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "                      sentence_words = nltk.word_tokenize(sentence_no_pun)\n",
        "\n",
        "                      # now we will check if in a sequence of words like w1 w2 \"χτυπάω\" w3 w4, a word like\n",
        "                      # \"εισητηριο\" is found. we will take the 2 previous and two following words if they exist\n",
        "                      # or else we take less words in the sequence\n",
        "                      min_word_ind = max(0, word_in_text-3)\n",
        "                      max_word_ind = min(word_in_text+4, len(sentence_words))\n",
        "\n",
        "                      before_and_after_words = sentence_words[min_word_ind:max_word_ind]\n",
        "\n",
        "                      for word_check in before_and_after_words:\n",
        "                          for ticket_string in ticket_strings:\n",
        "                              pattern = r\"\\b{}\\w*\\b\".format(ticket_string)\n",
        "                              if re.search(pattern, word_check, flags=re.IGNORECASE):\n",
        "                                  keep_sen = False\n",
        "                                  break\n",
        "                  if keep_sen:\n",
        "                    filtered_sentences.append(sentence)\n",
        "            else:\n",
        "                pattern = r\"\\b{}\\w*\\b\".format(keyword)\n",
        "                cond = re.search(pattern, sentence, flags=re.IGNORECASE)\n",
        "                if cond:\n",
        "                    filtered_sentences.append(sentence)\n",
        "                    break\n",
        "    return ' '.join(filtered_sentences)\n",
        "\n",
        "# starting from the dataframe texts we keep, only keep sentences which contain strings to keep\n",
        "def filter_sentences(text, keywords):\n",
        "    sentences = sent_tokenize(text)\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "        for keyword in keywords:\n",
        "            pattern = r\"\\b{}\\w*\\b\".format(keyword)\n",
        "            if re.search(pattern, sentence, flags=re.IGNORECASE):\n",
        "                filtered_sentences.append(sentence)\n",
        "                break\n",
        "    return ' '.join(filtered_sentences)\n",
        "\n",
        "# create and return a list of the words from the dataframe texts which\n",
        "# match with strings to keep. don't keep words that end with our keywords\n",
        "def find_matching_strings(text, mylist):\n",
        "    pattern = r'(?<=[\\s\\W])(?:' + '|'.join(map(re.escape, mylist)) + r')(?:\\w+)?'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS1XkfsRhqGe",
        "outputId": "54317251-bfbc-4d83-de4a-d6a786c93c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "filtered_df = filtered_df.copy()\n",
        "\n",
        "# keep only sentences with keywords\n",
        "# filtered_df['text'] = filtered_df['text'].apply(lambda x: filter_sentences(x, strings_law + strings_hurt))\n",
        "filtered_df['text'] = filtered_df['text'].apply(lambda x: filter_sentences_discard_tickets(x, strings_law + strings_hurt))\n",
        "\n",
        "# Add column that contains a list of words found in dataframe text that match with the strings to keep\n",
        "filtered_df['Matches'] = filtered_df['text'].apply(lambda x: find_matching_strings(x, strings_law + strings_hurt))\n",
        "\n",
        "# drop duplicates\n",
        "filtered_df=filtered_df.dropna()\n",
        "filtered_df = filtered_df.loc[filtered_df['text'] != \" \"]\n",
        "\n",
        "# Filter the dataframe to keep only rows that contain characters\n",
        "filtered_df['Contains_Characters'] = filtered_df['text'].apply(lambda x: np.any([c.isalpha() for c in str(x)]))\n",
        "filtered_df = filtered_df[filtered_df['Contains_Characters']]\n",
        "\n",
        "filtered_df = filtered_df.drop(['Contains_Characters'], axis=1)\n",
        "\n",
        "# print(filtered_df.shape)\n",
        "# filtered_df[:4]\n",
        "filtered_df\n",
        "# filtered_df.loc[3027]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ix8rJ334Ip5c",
        "outputId": "50660ccc-b321-462a-8dbb-9590ed37988b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "1     αγαπητοι κυριοι καλησπερα σας σημερα ωρα απογε...   \n",
              "16    παρακαλω πολυ σημερα κυριακη ωρα μμ λεωφορειο ...   \n",
              "43    aπο χθες πρωι υπαρχει ξεκιναει κανενα τρολει α...   \n",
              "62               πρεπει κινηθω εργασια λαικο νοσοκομειο   \n",
              "80    πρωι σαββατου εχοντας σχολασει απ τη δουλεια ε...   \n",
              "...                                                 ...   \n",
              "6593  καλημερα ονομαζομαι δαναη ορφανου τσωτα επικοι...   \n",
              "6618  γινει ρε οασα οδηγους σου καπνιζουν μεσα στα ο...   \n",
              "6622  εσταζε ολη οροφη νερα της βροχης διαδρομο τρεχ...   \n",
              "6623  απαραδεκτος οδηγος υπ αρθμον χεκ διασταυρωση δ...   \n",
              "6625  καλησπερα σας μεσα τρες μηνες δευτερο ατυχημα ...   \n",
              "\n",
              "                                              init_text  \\\n",
              "1     Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...   \n",
              "16    Παρακαλώ πολύ, σήμερα, Κυριακή, 30 /01 / 2022 ...   \n",
              "43    Aπό χθες το πρωι δεν υπαρχει και δεν ξεκιναει ...   \n",
              "62    Πρέπει να μετά κινηθώ στη εργασία μου στο Λαϊκ...   \n",
              "80    Το πρωί του Σαββάτου, έχοντας σχολασει απ' τη ...   \n",
              "...                                                 ...   \n",
              "6593  Καλημέρα, ονομάζομαι Δανάη Ορφανού Τσώτα. Επικ...   \n",
              "6618  Τι θα γίνει ρε ΟΑΣΑ με τους οδηγούς σου που κα...   \n",
              "6622  Έσταζε όλη η οροφή από τα νερά της βροχής στον...   \n",
              "6623  ΑΠΑΡΑΔΕΚΤΟΣ ΟΔΗΓΟΣ ΤΟΥ ΥΠ ΑΡΘΜΟΝ ΧΕΚ 6168 ΣΤΗΝ...   \n",
              "6625  Καλησπέρα σας,\\n\\nΕίναι μέσα σε τρες μήνες το ...   \n",
              "\n",
              "                                                Matches  \n",
              "1                                        [αποζημιωσετε]  \n",
              "16                                             [μηνυση]  \n",
              "43                                        [αποζημιωσει]  \n",
              "62                                         [νοσοκομειο]  \n",
              "80                                             [χτυπαω]  \n",
              "...                                                 ...  \n",
              "6593                            [χτυπησα, τραυματιστει]  \n",
              "6618                                           [μηνυση]  \n",
              "6622                                        [χτυπησεις]  \n",
              "6623                              [δικαιοσυνη, νομικες]  \n",
              "6625  [χτυπησα, αγωγη, χτυπησε, χτυπησα, χτυπημενο, ...  \n",
              "\n",
              "[622 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9a28964-4074-4a81-b72b-4f645876b96b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>init_text</th>\n",
              "      <th>Matches</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>αγαπητοι κυριοι καλησπερα σας σημερα ωρα απογε...</td>\n",
              "      <td>Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...</td>\n",
              "      <td>[αποζημιωσετε]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>παρακαλω πολυ σημερα κυριακη ωρα μμ λεωφορειο ...</td>\n",
              "      <td>Παρακαλώ πολύ, σήμερα, Κυριακή, 30 /01 / 2022 ...</td>\n",
              "      <td>[μηνυση]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>aπο χθες πρωι υπαρχει ξεκιναει κανενα τρολει α...</td>\n",
              "      <td>Aπό χθες το πρωι δεν υπαρχει και δεν ξεκιναει ...</td>\n",
              "      <td>[αποζημιωσει]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>πρεπει κινηθω εργασια λαικο νοσοκομειο</td>\n",
              "      <td>Πρέπει να μετά κινηθώ στη εργασία μου στο Λαϊκ...</td>\n",
              "      <td>[νοσοκομειο]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>πρωι σαββατου εχοντας σχολασει απ τη δουλεια ε...</td>\n",
              "      <td>Το πρωί του Σαββάτου, έχοντας σχολασει απ' τη ...</td>\n",
              "      <td>[χτυπαω]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6593</th>\n",
              "      <td>καλημερα ονομαζομαι δαναη ορφανου τσωτα επικοι...</td>\n",
              "      <td>Καλημέρα, ονομάζομαι Δανάη Ορφανού Τσώτα. Επικ...</td>\n",
              "      <td>[χτυπησα, τραυματιστει]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6618</th>\n",
              "      <td>γινει ρε οασα οδηγους σου καπνιζουν μεσα στα ο...</td>\n",
              "      <td>Τι θα γίνει ρε ΟΑΣΑ με τους οδηγούς σου που κα...</td>\n",
              "      <td>[μηνυση]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6622</th>\n",
              "      <td>εσταζε ολη οροφη νερα της βροχης διαδρομο τρεχ...</td>\n",
              "      <td>Έσταζε όλη η οροφή από τα νερά της βροχής στον...</td>\n",
              "      <td>[χτυπησεις]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6623</th>\n",
              "      <td>απαραδεκτος οδηγος υπ αρθμον χεκ διασταυρωση δ...</td>\n",
              "      <td>ΑΠΑΡΑΔΕΚΤΟΣ ΟΔΗΓΟΣ ΤΟΥ ΥΠ ΑΡΘΜΟΝ ΧΕΚ 6168 ΣΤΗΝ...</td>\n",
              "      <td>[δικαιοσυνη, νομικες]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6625</th>\n",
              "      <td>καλησπερα σας μεσα τρες μηνες δευτερο ατυχημα ...</td>\n",
              "      <td>Καλησπέρα σας,\\n\\nΕίναι μέσα σε τρες μήνες το ...</td>\n",
              "      <td>[χτυπησα, αγωγη, χτυπησε, χτυπησα, χτυπημενο, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>622 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9a28964-4074-4a81-b72b-4f645876b96b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9a28964-4074-4a81-b72b-4f645876b96b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9a28964-4074-4a81-b72b-4f645876b96b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conversion to JSON and save a json file of dictionaries"
      ],
      "metadata": {
        "id": "9M1ZrZaeZHc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# gets a list of strings and a string and returns the number of occurences of the string in the list\n",
        "def count_occurrences(string_list, target_string):\n",
        "    count = 0\n",
        "    for string in string_list:\n",
        "        if string == target_string:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# create a list of the json data as should be for spacy\n",
        "def dataframe_to_json(df):\n",
        "    json_list = []\n",
        "    for index, row in df.iterrows():\n",
        "        json_dict = {'content': row['text']}\n",
        "\n",
        "        list_of_labels = []\n",
        "        # some words exist in a list twice\n",
        "        list_of_checked_words = []\n",
        "        # the words in the texts that match with our keywords\n",
        "        for word in row['Matches']:\n",
        "\n",
        "          counted_already = count_occurrences(list_of_checked_words, word)\n",
        "\n",
        "          # C H E C K\n",
        "          start, end = ret_beginning_and_end_of_kth_occurence(row['text'], word, counted_already+1)\n",
        "\n",
        "          list_of_checked_words.append(word)\n",
        "\n",
        "          # assign the label as Hurt and if find that it is\n",
        "          # Law, change it\n",
        "          label_dict = {\"label\": [\"B-HUR\"]}\n",
        "\n",
        "          for keyword in strings_law:\n",
        "\n",
        "            pattern = r\"\\b{}\\w*\\b\".format(keyword)\n",
        "\n",
        "            if re.search(pattern, word, flags=re.IGNORECASE):\n",
        "              label_dict = {\"label\": [\"B-LAW\"]}\n",
        "              break\n",
        "\n",
        "          label_dict[\"points\"] = [\n",
        "              {\n",
        "              'text': str(word),\n",
        "              'start': start,\n",
        "               'end': end\n",
        "              }\n",
        "          ]\n",
        "\n",
        "          list_of_labels.append(label_dict)\n",
        "\n",
        "        json_dict[\"annotation\"] = list_of_labels\n",
        "        json_list.append(json.dumps(json_dict))\n",
        "    return json_list\n",
        "\n",
        "json_list = dataframe_to_json(filtered_df)\n",
        "\n",
        "for i in json_list[:2]:\n",
        "  json_object = json.loads(i)\n",
        "  json_formatted_str = json.dumps(json_object, indent=2, ensure_ascii=False)\n",
        "  print(json_formatted_str)\n",
        "\n",
        "# save the json file\n",
        "with open('/content/drive/My Drive/arximhdhs/ner/my_json_data.json', \"w\", encoding=\"utf-8\") as file:\n",
        "  for inner_dict in json_list:\n",
        "    json_object = json.loads(inner_dict)\n",
        "    json_data = json.dumps(json_object, ensure_ascii=False)\n",
        "    file.write(json_data)\n",
        "    file.write('\\n')"
      ],
      "metadata": {
        "id": "A5r2KJNMvE20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aad44bd-a8ab-4d3b-9a85-5493da8e22e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"content\": \"αγαπητοι κυριοι καλησπερα σας σημερα ωρα απογευμα κορη ετων βρισκοταν τερμα σαρωνιδα προορισμο λαγονησι δρομολογιο εκτελεστηκε θεση βρισκοταν υπ αριθμον χεη λεωφορειο σταθμευμενο εμπροσθεν της τασης πραγματοποιωντας λεγομενα οδηγου διαλειμμα οδηγος αγενεστατος οταν ρωτηθηκε κορη ωρα φυγει αρκεστηκε πει λεπτα εχοντας κλειστες πορτες αφηνοντας κοσμο κρυο οδηγος βρισκοταν δυο ατομα εντος λεωφορειου μιλαγε οδηγος ταξι πραγματοποιησα δρομολογιο βαρκιζα σαρωνιδα χωρις δρομο συναντησω αρκετα λεπτα αργοτερα υπ αριθμον χεη πραγματοποιησε κανονικα δρομολογιο ευγενεστατο οδηγο αγαπητοι κυριοι οασα της οσοι πολιτικη σας αφηνετε παιδια μεσα κρυο ακομα λεωφορειο κανει διαλειμμα κορη εχασε μαθημα λαγονησι εγω ετρεξα προλαβω αποζημιωσετε εχετε σκοπο φτιαξετε τη γραμμη μεγαλη αγανακτηση μαγκας κωνσταντινος διαθεση σας φωτογραφια λεωφορειο κανει διαλειμμα μπροστα σταση τερματος κατι τη γνωμη απαραδεκτη\",\n",
            "  \"annotation\": [\n",
            "    {\n",
            "      \"label\": [\n",
            "        \"B-LAW\"\n",
            "      ],\n",
            "      \"points\": [\n",
            "        {\n",
            "          \"text\": \"αποζημιωσετε\",\n",
            "          \"start\": 719,\n",
            "          \"end\": 730\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  \"content\": \"παρακαλω πολυ σημερα κυριακη ωρα μμ λεωφορειο αριθμο δρομολογιου κατευθυνση ζηρινειο προς αγιο στεφανο αττικης σταματησε σταση της λεωφορου θησεως εκαλη πλατεια βασιλεως παυλου παρολο εκανε σημα κορη οποια περιμενε μεσα νυχτα παγωνια αντιθετως οδηγος ετρεχε υπερβολικη παρανομη ταχυτητα σταματησε παρακαλω πολυ οπως ενημερωσετε στοιχεια συγκεκριμενου οδηγου συγκεκριμενο δρομολογιο γιατι ασκησω μηνυση εναντιον ασκωντας ολα νομιμα δικαιωματα επιφυλασσομενη καθοσον δικηγορος τοσο δυσκολους καιρους ζουμε ενοψει τοσο δυσκολων καιρικων συνθηκων τετοιες συμπεριφορες απαραδεκτες αδιανοητες ευχαριστω πολυ ελενη πανταζη\",\n",
            "  \"annotation\": [\n",
            "    {\n",
            "      \"label\": [\n",
            "        \"B-LAW\"\n",
            "      ],\n",
            "      \"points\": [\n",
            "        {\n",
            "          \"text\": \"μηνυση\",\n",
            "          \"start\": 395,\n",
            "          \"end\": 400\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert the above data into format needed by spaCy and save in a pickle file"
      ],
      "metadata": {
        "id": "laNcP5LsqKEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "def conversion_function(input_file=None, output_file=None):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        # with open(input_file, 'r', encoding=\"utf-8\") as f:\n",
        "        with open(input_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            # data = json.dumps(data, ensure_ascii=False)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            for annotation in data['annotation']:\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "                for label in labels:\n",
        "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        with open(output_file, 'wb') as fp:\n",
        "            pickle.dump(training_data, fp)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "conversion_function('/content/drive/My Drive/arximhdhs/ner/my_json_data.json', '/content/drive/My Drive/arximhdhs/ner/my_json_pkl.pkl')"
      ],
      "metadata": {
        "id": "yzjMH_vrqBsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## now our data is ready and we can train a NER model"
      ],
      "metadata": {
        "id": "iE_Dvw8eTvNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "# pip install -U 'spacy[cuda-autodetect]'\n",
        "\n",
        "import spacy.cli\n",
        "print()\n",
        "print(spacy.__version__)\n",
        "print()\n",
        "spacy.cli.download(\"el_core_news_sm\")\n",
        "# python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "w013Cyhmyj6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b1e34e-4830-450b-8d0b-c375af2752fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "Successfully installed spacy-3.5.3\n",
            "\n",
            "3.5.3\n",
            "\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('el_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model, or create an empty model using spacy.blank with the ID of desired language. If a blank model is being used, we have to add the entity recognizer to the pipeline. If an existing model is being used, we have to disable all other pipeline components during training using nlp.disable_pipes. This way, only the entity recognizer gets trained. Add the new entity label to the entity recognizer using the add_label method."
      ],
      "metadata": {
        "id": "g_uSgvIe1zSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training additional entity types using spaCy\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "with open ('/content/drive/My Drive/arximhdhs/ner/my_json_pkl.pkl', 'rb') as fp:\n",
        "    TRAIN_DATA = pickle.load(fp)\n",
        "\n",
        "\n",
        "# Test the trained model\n",
        "def test_trained_model(df, column, num, my_nlp):\n",
        "  print()\n",
        "  test_text = df[column].iloc[num]\n",
        "  doc = my_nlp(test_text)\n",
        "  print(\"Entities in '%s'\" % test_text)\n",
        "  for ent in doc.ents:\n",
        "      print(ent.label_, ent.text)\n",
        "\n",
        "\n",
        "def train_my_model(model=None, new_model_name='our_NER_model', output_dir=None, n_iter=26):\n",
        "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
        "    nlp = spacy.load(model)\n",
        "    print(\"Loaded model '%s'\" % model)\n",
        "\n",
        "    batch_size = 128\n",
        "    drop_out = 0.15\n",
        "    learning_rate = 0.001\n",
        "    regularization = 1e-6\n",
        "    grad_clip_value = 1.0\n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    else:\n",
        "        ner = nlp.get_pipe('ner')\n",
        "        print(\"ner \", ner)\n",
        "\n",
        "    # # Specify the new entity labels which you want to add here\n",
        "    # LABEL = [\"B-HUR\", \"B-LAW\"]\n",
        "    # # Add new entity labels to entity recognizer\n",
        "    # for i in LABEL:\n",
        "    #     ner.add_label(i)\n",
        "    # Add your custom labels to the NER component\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # Get names of other pipes to disable them during training to train only NER\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "\n",
        "        if model is None:\n",
        "            optimizer = nlp.begin_training()\n",
        "        else:\n",
        "            optimizer = ner.create_optimizer()\n",
        "\n",
        "        optimizer.learn_rate = learning_rate\n",
        "        optimizer.L2 = regularization\n",
        "        optimizer.grad_clip = grad_clip_value\n",
        "\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "\n",
        "            for batch in minibatch(TRAIN_DATA, size=batch_size):\n",
        "              for text, annotations in batch:\n",
        "                  doc = nlp.make_doc(text)\n",
        "\n",
        "                  example = Example.from_dict(doc, annotations)\n",
        "                  # the drop parameter controls the dropout rate, which helps prevent overfitting\n",
        "                  # nlp.update([example], losses=losses, drop=drop_out)  # Update the NER model\n",
        "                  nlp.update([example], losses=losses, drop=drop_out, sgd=optimizer)  # Update the NER model\n",
        "\n",
        "            print(f\"Iteration {itn+1}: Train Losses {losses}\")\n",
        "\n",
        "    test_trained_model(filtered_df, 'text', 145, nlp)\n",
        "    test_trained_model(filtered_df, 'text', 45, nlp)\n",
        "    test_trained_model(filtered_df, 'text', 67, nlp)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # Save model\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.meta['name'] = new_model_name  # rename model\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # Test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        test_trained_model(filtered_df, 'text', 34, nlp2)"
      ],
      "metadata": {
        "id": "KhD2t3Ul_ObW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_my_model('el_core_news_sm', 'our_NER_model', '/content/drive/My Drive/arximhdhs/ner/new_our_NER_model', 26)"
      ],
      "metadata": {
        "id": "kMs0XXOIS6Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e46d71-af3b-4259-c276-097a9274ecc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model 'el_core_news_sm'\n",
            "ner  <spacy.pipeline.ner.EntityRecognizer object at 0x7f3d820f68f0>\n",
            "Iteration 1: Train Losses {'ner': 704.2147681896428}\n",
            "Iteration 2: Train Losses {'ner': 160.55492481198218}\n",
            "Iteration 3: Train Losses {'ner': 75.33999263396632}\n",
            "Iteration 4: Train Losses {'ner': 46.08789793582352}\n",
            "Iteration 5: Train Losses {'ner': 25.716718812045233}\n",
            "Iteration 6: Train Losses {'ner': 29.394540499555422}\n",
            "Iteration 7: Train Losses {'ner': 44.536981704031035}\n",
            "Iteration 8: Train Losses {'ner': 11.476509322398973}\n",
            "Iteration 9: Train Losses {'ner': 32.19116715104209}\n",
            "Iteration 10: Train Losses {'ner': 41.088112039414526}\n",
            "Iteration 11: Train Losses {'ner': 34.41297370137192}\n",
            "Iteration 12: Train Losses {'ner': 16.890464505195197}\n",
            "Iteration 13: Train Losses {'ner': 11.849082397873499}\n",
            "Iteration 14: Train Losses {'ner': 3.420062469573205}\n",
            "Iteration 15: Train Losses {'ner': 10.345500249155172}\n",
            "Iteration 16: Train Losses {'ner': 21.908732797937468}\n",
            "Iteration 17: Train Losses {'ner': 14.439387569214613}\n",
            "Iteration 18: Train Losses {'ner': 0.21412960930208202}\n",
            "Iteration 19: Train Losses {'ner': 12.080650988933401}\n",
            "Iteration 20: Train Losses {'ner': 4.025550381598508}\n",
            "Iteration 21: Train Losses {'ner': 8.499442745742533}\n",
            "Iteration 22: Train Losses {'ner': 13.113595852377475}\n",
            "Iteration 23: Train Losses {'ner': 10.097256177542237}\n",
            "Iteration 24: Train Losses {'ner': 14.252105572689572}\n",
            "Iteration 25: Train Losses {'ner': 6.058899548098982}\n",
            "Iteration 26: Train Losses {'ner': 0.017468062040479227}\n",
            "\n",
            "Entities in 'απαραδεκτη σημερινη καθυστερηση μετρο αργυρουπολης ερθει ηρθε μεγαλος συνωστισμος εις βαρος της υγειας καθυστερηση εργασια ποιος αποζημιωσει'\n",
            "B-LAW αποζημιωσει\n",
            "\n",
            "Entities in 'ωρα περιπου επιβιβαστηκα λεωφορειο α σταση ευαγγελιστρια προορισμο αγ σκεπη αποβιβαση οδηγος κλεινοντας ενα φυλλο της πορτας διοτι αλλο ηταν προφανως χαλασμενο γιατι ανοιγοκλεινε τραυματισε δαχτυλο αριστερου ποδιου καθως πορτα εκλεινε ταυτοχρονα αποβιβαση επιεικως απαραδεκτη εικονα λεωφορειου οποιο ηταν σχεδον διαλυμενο'\n",
            "B-HUR τραυματισε\n",
            "\n",
            "Entities in 'οπως ξεκινησε λεωφορειο σταση πισω μερος βρηκε οροφη της στασης εξω υπ μεταφορων χολαργου ρευμα προς γεφυρα σταυρου αποτελεσμα της σπασειαφου εδωσε καμια σημασια συνεχισε πεταξε ολη οροφη μαζι διαφημιστικο κατω κινδυνο θανατηφορου ατυχηματος πεζωνδεν σταματησε καν σηκωθηκε εφυγε χωρις κανενα ενδιαφερον χτυπησε καποιοσυπαρχει περιπτερο καμερες καταγραψει ολο συμβαν'\n",
            "B-HUR χτυπησε\n",
            "\n",
            "\n",
            "Saved model to /content/drive/My Drive/arximhdhs/ner/new_our_NER_model\n",
            "Loading from /content/drive/My Drive/arximhdhs/ner/new_our_NER_model\n",
            "\n",
            "Entities in 'καλησπερα ηθελα σας καταγγειλω διαδρομη λεωφορειο αφετηρια αττικο νοσοκομειο προς νικαια συγκεκριμενα δρομολογιο αττικο σχεδον καθημερνη βαση υπαρξη της προστατευτικης μασκας ανυπαρκτη οδηγος παρατηρηση απαντησε εκεινος μπορει κανει τιποτα επειδη κατασταση σχεδον καθημερινοτητα ειδικα αυτες ωρες αναγκαζομαστε εργαζομενοι νοσοκομειου επισκεπτες νοσοκομειου μπορουμε παιρνουμε λεωφορειο πιο συγκεκριμενα γεγονος συνδεεται οπως φανηκε ληξη μαθηματων επαλ ευχαριστω'\n",
            "B-HUR νοσοκομειο\n",
            "B-HUR νοσοκομειου\n",
            "B-HUR νοσοκομειου\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the saved model\n",
        "output_dir = '/content/drive/My Drive/arximhdhs/ner/new_our_NER_model'\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "test_trained_model(filtered_df, 'text', 34, nlp2)\n",
        "test_trained_model(filtered_df, 'text', 43, nlp2)\n",
        "test_trained_model(filtered_df, 'text', 167, nlp2)"
      ],
      "metadata": {
        "id": "9NsFCgniS6kU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e90ef9-c071-4b1b-f063-308f82a100b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from /content/drive/My Drive/arximhdhs/ner/new_our_NER_model\n",
            "\n",
            "Entities in 'καλησπερα ηθελα σας καταγγειλω διαδρομη λεωφορειο αφετηρια αττικο νοσοκομειο προς νικαια συγκεκριμενα δρομολογιο αττικο σχεδον καθημερνη βαση υπαρξη της προστατευτικης μασκας ανυπαρκτη οδηγος παρατηρηση απαντησε εκεινος μπορει κανει τιποτα επειδη κατασταση σχεδον καθημερινοτητα ειδικα αυτες ωρες αναγκαζομαστε εργαζομενοι νοσοκομειου επισκεπτες νοσοκομειου μπορουμε παιρνουμε λεωφορειο πιο συγκεκριμενα γεγονος συνδεεται οπως φανηκε ληξη μαθηματων επαλ ευχαριστω'\n",
            "B-HUR νοσοκομειο\n",
            "B-HUR νοσοκομειου\n",
            "B-HUR νοσοκομειου\n",
            "\n",
            "Entities in 'καλησπερα σαςθα ηθελα καταγγειλω οδηγο λεωφορειου της κυκλικης διαδρομης πινακιδες yne ακολουθω καθημερινα τη γραμμη καθως κατευθυνομαι εργασια εχω γινει ματυρας ηδη δυο κακων συμβαντων συκεκριμενο ανευθυνο αγενη οδηγο τοσο προς μια κυρια προς προσωπο χτυπησαμε κανονικα κουμπια κανει σταση ωστοσο χτυπησε χαρακτηριστικος ηχος ειδοποιει οδηγοοταν φωναξα σταση απαντησεοτι κανει σταση πρεπει φτασει εξηγησα προσπερασε εντελως τη σταση μανωλη ειχα χτυπησει κουμπιμου απαντησε αγενεια φταιει εκεινος ξερω παταω ενα κουμπι ιδια συμπεριφορα επανελαβε προς κυρια σημερα ολο λεωφορειο εξηγει προφανως χαλασει κουμπι υποχρεωση τοσο γνωριζει πραγματοποιει στασειςπαρακαλω προς ελεγχο αρμοδιους τοσο οχημα οδηγο γιατι συνεχισει συμβαινει μπει ενα τελος γινει συσταση συμορφωσης εν λογω κυριο ευχαριστω εκ προτερων περιστατικα συνεβησαν πρωινη βαρδια'\n",
            "B-HUR χτυπησαμε\n",
            "B-HUR χτυπησε\n",
            "B-HUR χτυπησει\n",
            "\n",
            "Entities in 'καταγγελια απειλες καταχρηση εξουσιας ελεγκτη εισητηριων τρολευ αθηνα ολιγων ημερων συγκεκριμενο διπολο εκανε τηλεφωνικη κληση ανευ λογου ολιγων ημερων βρεθηκα αυτοπτης μαρτυρας μπροστα επιθεση δεχτηκαν επιβατες ελεγκτη εισητηριων μεσα τρολευ επειδη επιβατες θελησαν πριν γινει ελεγχος εισητηριων επιδειξει ελεγκτης ως υποχρεουται νομο επαγγελματικη ταυτοτητα υπενθυμιζεται δευτερος ουτως αλλως υποχρεουται νομο φερει ταμπελακι ονοματεπωνυμο επαγγελματικη ιδιοτητα ως ελεγκτης εισητηριων μεσα στα τρολευ στα λεοφωρεια εν συνεχεια ελεγκτης εισητηριων οχι μονο αρνηθηκε επιδειξει επαγγελματικη ταυτοτητα στους επιβατες απειλουσε τραμπουκιζε επιβατες μεσα τρολευ οταν επεμεναν επιδειξη της επαγγελματικης ταυτοτητας συγκεκριμενο διπολο καλεσε σκοπο εκφοβισει επιβατες επιβατες ουτως αλλως ειχαν επειδειξει εισητηρια παρολο ελεγκτης παρανομως αρνηθηκε γνωστοποιησει επαγγελματικη ταυτοτητα στους επιβατες οταν ζητηθηκε μονος παρανομος μεσα τρολευ ηταν ελεγκτης αυτος οποιου γινεται καταγγελια νομιμοποιουνται ελεγκτες εισητηριων μπαινοβγαινουν μεσα στα τρολευ στα λεωφορεια της αθηνας παρανομα επικαλουμενοι ισχυριζομενοι ελεγκτες χωρις φερουν πανω μπλουζα ως νομο υποχρεουνται επαγγελματικη ταυτοτητα ονοματεπωνυμο επαγγελματικη ιδιοτητα δικαιωμα διπολο τραμπουκισε επιβατες μεσα τρολευ αθηνα ολιγων ημερων αρνηθηκε επιδειξει επαγγελματικη ταυτοτητα στους επιβατες επομενη φορα υπαρξει περιστατικο ελεγκτες εισητηριων εισελθουν παρανομως μεσα στα τρολευ στα λεωφορεια χωρις επαγγελματικη ταυτοτητα οποια αναγραφει επαγγελματικη ιδιοτητα ονοματεπωνυμο βεβαιοι τοπου καλεστουν καναλια δημοσιοποιηση της καταχρησης εξουσιας τραμπουκισμου επιδιωκει κυβερνηση εναντιον επιβατων στα μμμ αναζητηθουν αυτεπαγγελτα εργολαβικες εταιρειες οποιες κρυβονται πισω επιθετικες καταχρηστικες συμπεριφορες ελεγκτων εισητηριων εναντιον επιβατων μεσα στα τρολευ στα λεωφορεια της αθηνας ενεργωντας παρανομα καταχρηστικα προς επιβατες'\n",
            "B-HUR επιθεση\n",
            "B-HUR επιθετικες\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the saved model to recognize the sos comments and save a csv that contains the complaints that are considered important"
      ],
      "metadata": {
        "id": "RStkX4TxVjOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sos_column(text, my_nlp):\n",
        "    # print(\"\\n\", text)\n",
        "    doc = my_nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        # print(ent.label_, ent.text)\n",
        "        if ent.label_ in [\"B-LAW\", \"B-HUR\"]:\n",
        "          return 1\n",
        "    return 0\n",
        "\n",
        "new_df = data.copy()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/arximhdhs/ner/new_our_NER_model'\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "new_df['SOS'] = data['text'].apply(lambda x: create_sos_column(x, nlp2))\n",
        "\n",
        "df_sos = new_df[new_df['SOS'] == 1]\n",
        "\n",
        "def create_ent_column(text, my_nlp):\n",
        "    doc = my_nlp(text)\n",
        "    return doc.ents\n",
        "\n",
        "final_df = df_sos.copy()\n",
        "\n",
        "final_df['ents'] = df_sos['text'].apply(lambda x: create_ent_column(x, nlp2))\n",
        "\n",
        "final_df = final_df.drop(['text'], axis=1)\n",
        "final_df = final_df.reset_index(drop = True)\n",
        "final_df.to_csv('/content/drive/My Drive/arximhdhs/ner/sos_comments_new.csv')\n",
        "final_df"
      ],
      "metadata": {
        "id": "ty5j9frzEX0V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "154c9654-1dd7-498e-e972-d1cff34a3fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             init_text  SOS  \\\n",
              "0    Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...    1   \n",
              "1    Παρακαλώ πολύ, σήμερα, Κυριακή, 30 /01 / 2022 ...    1   \n",
              "2    Aπό χθες το πρωι δεν υπαρχει και δεν ξεκιναει ...    1   \n",
              "3    Πρέπει να μετά κινηθώ στη εργασία μου στο Λαϊκ...    1   \n",
              "4    Το πρωί του Σαββάτου, έχοντας σχολασει απ' τη ...    1   \n",
              "..                                                 ...  ...   \n",
              "759  Καλημέρα, ονομάζομαι Δανάη Ορφανού Τσώτα. Επικ...    1   \n",
              "760  Τι θα γίνει ρε ΟΑΣΑ με τους οδηγούς σου που κα...    1   \n",
              "761  Έσταζε όλη η οροφή από τα νερά της βροχής στον...    1   \n",
              "762  ΑΠΑΡΑΔΕΚΤΟΣ ΟΔΗΓΟΣ ΤΟΥ ΥΠ ΑΡΘΜΟΝ ΧΕΚ 6168 ΣΤΗΝ...    1   \n",
              "763  Καλησπέρα σας,\\n\\nΕίναι μέσα σε τρες μήνες το ...    1   \n",
              "\n",
              "                                                  ents  \n",
              "0                                    ((αποζημιωσετε),)  \n",
              "1                                          ((μηνυση),)  \n",
              "2                                     ((αποζημιωσει),)  \n",
              "3                                      ((νοσοκομειο),)  \n",
              "4                                          ((χτυπαω),)  \n",
              "..                                                 ...  \n",
              "759                        ((χτυπησα), (τραυματιστει))  \n",
              "760                                        ((μηνυση),)  \n",
              "761                                     ((χτυπησεις),)  \n",
              "762                          ((δικαιοσυνη), (νομικες))  \n",
              "763  ((χτυπησα), (αγωγη), (χτυπησε), (χτυπησα), (χτ...  \n",
              "\n",
              "[764 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d261488-d447-43b8-9e3b-646eb3b31201\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_text</th>\n",
              "      <th>SOS</th>\n",
              "      <th>ents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Αγαπητοί κύριοι καλησπέρα σας σήμερα 31/1 2022...</td>\n",
              "      <td>1</td>\n",
              "      <td>((αποζημιωσετε),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Παρακαλώ πολύ, σήμερα, Κυριακή, 30 /01 / 2022 ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((μηνυση),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aπό χθες το πρωι δεν υπαρχει και δεν ξεκιναει ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((αποζημιωσει),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Πρέπει να μετά κινηθώ στη εργασία μου στο Λαϊκ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((νοσοκομειο),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Το πρωί του Σαββάτου, έχοντας σχολασει απ' τη ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((χτυπαω),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>Καλημέρα, ονομάζομαι Δανάη Ορφανού Τσώτα. Επικ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((χτυπησα), (τραυματιστει))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>Τι θα γίνει ρε ΟΑΣΑ με τους οδηγούς σου που κα...</td>\n",
              "      <td>1</td>\n",
              "      <td>((μηνυση),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>Έσταζε όλη η οροφή από τα νερά της βροχής στον...</td>\n",
              "      <td>1</td>\n",
              "      <td>((χτυπησεις),)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>ΑΠΑΡΑΔΕΚΤΟΣ ΟΔΗΓΟΣ ΤΟΥ ΥΠ ΑΡΘΜΟΝ ΧΕΚ 6168 ΣΤΗΝ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((δικαιοσυνη), (νομικες))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>Καλησπέρα σας,\\n\\nΕίναι μέσα σε τρες μήνες το ...</td>\n",
              "      <td>1</td>\n",
              "      <td>((χτυπησα), (αγωγη), (χτυπησε), (χτυπησα), (χτ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>764 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d261488-d447-43b8-9e3b-646eb3b31201')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d261488-d447-43b8-9e3b-646eb3b31201 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d261488-d447-43b8-9e3b-646eb3b31201');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}